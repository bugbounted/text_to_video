{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hai/anaconda3/envs/ttv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import altair as alt\n",
    "import math\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from diffusers.utils import export_to_video\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]Taking `'Attention' object has no attribute 'key'` while using `accelerate.load_checkpoint_and_dispatch` to mean /Users/hai/.cache/huggingface/hub/models--damo-vilab--text-to-video-ms-1.7b/snapshots/338683c3175cde59b407a699af8fe68f4a2b8f74/vae was saved with deprecated attention block weight names. We will load it with the deprecated attention block names and convert them on the fly to the new attention block format. Please re-save the model after this conversion, so we don't have to do the on the fly renaming in the future. If the model is from a hub checkpoint, please also re-upload it or open a PR on the original repository.\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:02<00:00,  1.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextToVideoSDPipeline {\n",
       "  \"_class_name\": \"TextToVideoSDPipeline\",\n",
       "  \"_diffusers_version\": \"0.20.0.dev0\",\n",
       "  \"_name_or_path\": \"damo-vilab/text-to-video-ms-1.7b\",\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"DPMSolverMultistepScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet3DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\", device_map = 'auto')\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = 'The dog is dancing'\n",
    "num_inference_steps =  25\n",
    "num_frames = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m video_frames \u001b[39m=\u001b[39m pipe(prompt_text, num_inference_steps\u001b[39m=\u001b[39;49mnum_inference_steps,num_frames\u001b[39m=\u001b[39;49mnum_frames)\u001b[39m.\u001b[39mframes\n",
      "File \u001b[0;32m~/anaconda3/envs/ttv/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ttv/lib/python3.9/site-packages/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py:564\u001b[0m, in \u001b[0;36mTextToVideoSDPipeline.__call__\u001b[0;34m(self, prompt, height, width, num_frames, num_inference_steps, guidance_scale, negative_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, output_type, return_dict, callback, callback_steps, cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[39m# 3. Encode input prompt\u001b[39;00m\n\u001b[1;32m    561\u001b[0m text_encoder_lora_scale \u001b[39m=\u001b[39m (\n\u001b[1;32m    562\u001b[0m     cross_attention_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mscale\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mif\u001b[39;00m cross_attention_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    563\u001b[0m )\n\u001b[0;32m--> 564\u001b[0m prompt_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_prompt(\n\u001b[1;32m    565\u001b[0m     prompt,\n\u001b[1;32m    566\u001b[0m     device,\n\u001b[1;32m    567\u001b[0m     num_images_per_prompt,\n\u001b[1;32m    568\u001b[0m     do_classifier_free_guidance,\n\u001b[1;32m    569\u001b[0m     negative_prompt,\n\u001b[1;32m    570\u001b[0m     prompt_embeds\u001b[39m=\u001b[39;49mprompt_embeds,\n\u001b[1;32m    571\u001b[0m     negative_prompt_embeds\u001b[39m=\u001b[39;49mnegative_prompt_embeds,\n\u001b[1;32m    572\u001b[0m     lora_scale\u001b[39m=\u001b[39;49mtext_encoder_lora_scale,\n\u001b[1;32m    573\u001b[0m )\n\u001b[1;32m    575\u001b[0m \u001b[39m# 4. Prepare timesteps\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mset_timesteps(num_inference_steps, device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/ttv/lib/python3.9/site-packages/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py:256\u001b[0m, in \u001b[0;36mTextToVideoSDPipeline._encode_prompt\u001b[0;34m(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, prompt_embeds, negative_prompt_embeds, lora_scale)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m         attention_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     prompt_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_encoder(\n\u001b[0;32m--> 256\u001b[0m         text_input_ids\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m    257\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    258\u001b[0m     )\n\u001b[1;32m    259\u001b[0m     prompt_embeds \u001b[39m=\u001b[39m prompt_embeds[\u001b[39m0\u001b[39m]\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_encoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ttv/lib/python3.9/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "video_frames = pipe(prompt_text, num_inference_steps=int(num_inference_steps),num_frames=int(num_frames)).frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
